# ğŸ“œ BibleGPT

**BibleGPT** is a character-level transformer trained from scratch on the full text of the King James Bible. Inspired by Andrej Karpathyâ€™s â€œGPT from scratchâ€ tutorial, this project builds a miniature language model capable of generating biblical-sounding text â€” from divine proclamations to righteous hallucinations.

> _â€œAnd it came to pass, the model did utter forth tokens most holy and strange.â€_

---

## âœ¨ Features

- Built entirely from scratch using PyTorch
- Trains on cleaned King James Bible text (`bible.txt`)
- Implements a Transformer model (self-attention, positional encoding, etc.)
- Generates new "verses" in the style of biblical scripture
- Easily extensible or repurposable for other stylized datasets

---

## ğŸ“ Project Structure

```
BibleGPT/
â”œâ”€â”€ bible.txt           # Cleaned training data (KJV)
â”œâ”€â”€ prepare.py          # Tokenize and binarize data
â”œâ”€â”€ train.py            # Main training loop
â”œâ”€â”€ generate.py         # Text generation script
â”œâ”€â”€ transformer.py      # Model architecture (Transformer)
â”œâ”€â”€ config.py           # Model & training hyperparameters
â”œâ”€â”€ utils.py            # (Optional) helper functions
â”œâ”€â”€ train.bin / val.bin # Binary encoded datasets
â”œâ”€â”€ meta.pkl            # Vocabulary + mappings
â”œâ”€â”€ README.md           # This file
```

---

## ğŸ§  How It Works

1. `prepare.py` tokenizes the Bible at the character level and creates training/validation splits.
2. `transformer.py` defines the architecture: multi-head attention, feedforward layers, and positional encoding.
3. `train.py` performs gradient descent on batches to minimize cross-entropy loss.
4. `generate.py` uses the trained model to produce new text, character-by-character.

---

## ğŸš€ Getting Started

### ğŸ§± Install Dependencies

```bash
pip install torch numpy tqdm
```

(Optional: use a virtual environment)

### ğŸ› ï¸ Prepare Data

```bash
python prepare.py
```

### ğŸ§  Train the Model

```bash
python train.py
```

To train for more steps, adjust `config.py` or pass args via CLI.

### ğŸ”® Generate Text

```bash
python generate.py
```

---

## ğŸ§ª Sample Output

> _"And he went forth into the land of Moab, and said unto the house of Israel, Surely I will bring thee out of the wilderness."_  
>  
> _"Let not the hand of the Lord be turned away, for in the seventh hour there shall be a sign among the children of men."_

(May also produce hilariously incoherent gibberish at early training stages â€” it's part of the fun.)

---

## ğŸ§° Tips

- Use `caffeinate` on macOS to prevent sleep during training:
  ```bash
  caffeinate python train.py
  ```
- Training for **10kâ€“50k+ steps** is recommended for coherent output.
- Model checkpoints and bin files can be large â€” donâ€™t commit them to GitHub.

---

## ğŸ› ï¸ Future Ideas

- Add verse/chapter-style formatting to generations
- Fine-tune on other stylized texts (e.g., Shakespeare, philosophical texts)
- Host a demo via FastAPI or Flask
- Embed into your personal website with a frontend UI

---

## ğŸ™ Credits

- Based on [Andrej Karpathyâ€™s "Let's build GPT"](https://youtu.be/kCc8FmEb1nY)
- Dataset: [King James Bible (Project Gutenberg)](https://www.gutenberg.org/ebooks/10)

---

## âš ï¸ Disclaimer

This is a personal/educational project. The output is not intended to be taken as real scripture or theological guidance. For entertainment and experimentation only.

---